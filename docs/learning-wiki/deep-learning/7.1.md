# LeNet, AlexNet, and VGG
I'll be going over [AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf),
one of the most influential papers on Convolutional Neural Networks

- Designed by Alex Krizhevsky with Ilya Sutskever and Geoffrey Hinton in 2012
- Achieved state of the art results on the LSVRC-2010 ImageNet

# Architecture

- While from afar, AlexNet can just be seen as a much larger [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf), the are various advancements made in its architecture that made its performance so much better

## Nonlinearity
- The [__ReLU__](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) function was chosen over
the $\tanh$ function as the nonlinear activation
- Unlike the $\text{sigmoid}$ or $\tanh$ functions, __ReLU__ does not __saturate__:
    - In this context, saturation is when functions output values close to their __asymptotic ends__; 
    which are $0$ as $x \rightarrow -\infty, 1$ $x \rightarrow \infty$ and $-1$ as $x \rightarrow -\infty, 1$ $x \rightarrow \infty$  for $\text{sigmoid}$ and $\tanh$, respectively
    - Saturation squashes the derivitive leading to smaller steps taken during gradient descent $\implies$ slower training

![](img/tanh_relu.png#center){width=70%}


## Overlapping Pooling
- Max Pooling is an operation frequently used in CNNs to 
- One choice in this model was to use __overlapping pooling__
- 

## Architecture
![AlexNet Architecture](https://www.researchgate.net/profile/Nicola-Strisciuglio/publication/339756908/figure/fig5/AS:866265283457032@1583545146587/AlexNet-architecture-used-as-the-baseline-model-for-the-analysis-of-results-on-the.png#center){ width=100% }