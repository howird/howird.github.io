3\. Adaboost
============

(1) In a random forest (RF), always make a full-sized tree. In a forest of trees made in Adaboost, the trees are usually just a stump (a node + 2 leaves)

Stumps are not great at making accurate classifications (weak learners). However, that’s the way Adaboost likes it, and it’s one of the reasons why they’re so commonly combined.

(2) In RF, each tree has an equal vote on the final classification. In a forest of stumps made with Adaboost, some get more say in the final classification than the others.

(3) In RF, each decision tree is made independently of the others. It doesn’t matter if the tree was made first/last. In Adaboost, the order is important. The error that the first stump makes influences how the second stump makes…

![](https://miro.medium.com/max/1400/1*zKt0Cyv3hfoLbHH0zs2YPQ.png)

Step 1: Give each sample a weight that indicates how important it is to be correctly classified

![](https://miro.medium.com/max/1400/1*2nqCiyXQC0jluNfg6ixmsw.png)

After we make the first stump, these weights will change in order to guide how the next stump is created.

Step 2: Make the first stump in the forest. This is done by finding the variable Chest Pain, Blocked Arteries, or Patient Weight, that does the best job classifying the samples.

![](https://miro.medium.com/max/1400/1*jX_1rYTF08cg7K6w_Z3u4Q.png)

Step 3: Calculate Gini Index for three stumps

![](https://miro.medium.com/max/1400/1*w-qoRb8lcn20B2Aln8Kybg.png)

\=> Patient Weight will be the first stump in the forest.

Step 4: Determine **how much say a stump has** in the final classification based on how well it classified the samples

The Total Error (\\in \[0,1\]) for a stump is the sum of the weights associated with the incorrectly classified samples.

![](https://miro.medium.com/max/1400/1*KT3RCyCfoqFeWN1ey5sKxQ.png)167 < 176 => No Heart Disease. But the label is Yes. => Total error = 1/8

We use the Total Error to determine the Amount of Say this stump has in the final classification with the following formula:

![](https://miro.medium.com/max/1400/1*GxqqQjuM0WCHjsJp-2EAiA.png)\=0.97 for Weight stump. y-axis: Amount of Say

*   When a stump does a good job, and the Total Error is small, then the Amount of Say is a relatively large, positive value.
*   When a stump is no better at classification than flipping a coin (i.e. half of the samples are correctly classified and half are incorrectly classified) and Total Error = 0.5, then the Amount of Say = 0.
*   When a stump does a terrible job and the Total Error = 1, in other words, if the stump consistently gives you the opposite classification, then the Amount of Say will be a large negative value. So if a stump votes for “Heart Disease”, the negative Amount of Say will vote into “Not Heart Disease”.

Step 5: Modify the weights so that the next stump will take the errors that the current stump into account.

Since the Weight variable stump **incorrectly** classified sample 4, we will emphasize the need for the next stump to correctly classify it by i**ncreasing its sample weight & decreasing all of the other sample weights.**

(1) Increase sample weight for incorrectly classified sample

![](https://miro.medium.com/max/1400/1*DE2ZCZFpYse-PEyIk5PxoQ.png)

*   When the Amount of Say is relatively large (i.e. the last stump did a great job classifying samples), then we’ll scale the previous sample weight with a large number.
*   When the Amount of Say is relatively low (i.e. the last stump did not do a very good job classifying samples), then the previous Sample Weight is scaled by a relatively small number. This means the New Sample Weight will only be a little larger than the old one.

![](https://miro.medium.com/max/1400/1*oCSSvReD54KXPU69rxs_6g.png)

(2) Decrease the Sample Weights for all of the correctly classified samples

![](https://miro.medium.com/max/1400/1*2KHmIE-iIsapOcgY1dw1Dg.png)

*   When the amount of say is relatively large, scale the sample weight by a value very close to 0. This will make the New Sample Weight very small.
*   When the amount of say is relatively small, then we will scale the Sample Weight by a value close to 1. That means that the New Sample Weight will be just a little smaller than the old one.

![](https://miro.medium.com/max/1400/1*JEyGCsdbktpy2zubmmyqnw.png)![](https://miro.medium.com/max/1400/1*hNykcdGvh2voOOP95bTGEg.png)Normalize the weight

Step 6: Use the modified sample weights to make the 2nd stump

In theory, we could use the sample weights to calculate the **weighted Gini Index** to determine which variable should split the next stump. The weighted GI would put more emphasis on correctly classifying sample 4 (the one that was misclassified by the last stump) since it has the largest sample weight.

Alternatively, instead of using weighted GI, we can make a new collection of samples that contains duplicate copies of the samples with the largest **sample weight.**

(1) Making a new, empty dataset that is the same size as original

(2) Pick a random # in \[0,1\] and see where the # falls when we use the sample weights like a distribution.

0.07 <- If the # in \[0,0.07\], put this sample into new collection of samples

0.07<- If the # in (0.07,0.14\], put this sample into new collection of samples

0.07<- If the # in (0.14,0.21\], put this sample into new collection of samples

0.49<- If the # in (0.21,0.70\], put this sample into new collection of samples

0.07<- If the # in (0.70,0.77\], put this sample into new collection of samples

…

(3) Pick random numbers and add samples to the new collection until the new collection is the same size as the original.

![](https://miro.medium.com/max/1400/1*5jom3chVjwieI5fw4dw4Zw.png)

(4) Get rid of the original samples and use the new collection of samples. Lastly, we give all of the samples equal sample weights just like before.

![](https://miro.medium.com/max/1400/1*sNcc6Ay9LGowLGO71rJVAw.png)

Because these samples are all the same, they will be treated as a block, creating a large penalty for being misclassified.

(5) Find the root node for the 2nd stump

![](https://miro.medium.com/max/1400/1*jGIot-e7bC3eGt0Ty79-OQ.png)

Step 7: When we have all the stumps

![](https://miro.medium.com/max/1400/1*ygjDxG-OpOvmA-U0YE7Tyg.png)

