2\. Random Forest
=================

2.1 Building & Evaluating Random Forest
---------------------------------------

*   Disadvantage of Decision Trees: ‚ÄúTrees have one aspect that prevents them from being the ideal tool for predictive learning, namely, inaccuracy.‚Äù ‚Äî The Element of Statistical Learning
*   In other words, they work great with the data to create them, but **they are not flexible when it comes to classifying new samples.**

The random forest combines the simplicity of decision trees with flexibility resulting in a vast improvement in accuracy.

_Step 1: Create a bootstrapped dataset_

To create a bootstrapped dataset that is the same size as the original, we just randomly select samples from the original dataset. (Note: We‚Äôre able to pick the same sample more than once.)

_Step 2: Create a decision tree using the bootstrapped dataset, but only use a random subset of variables (columns, can be selected multiple times in a tree) at each step_

![](https://miro.medium.com/max/1400/1*y7XcfEscSNz4cHKUwEmxVw.png)![](https://miro.medium.com/max/1400/1*MxAsoNZ_Sze0SLb7HImkTA.png)

Now go back to the first step and repeat: Make a bootstrapped dataset and build a tree considering a subset of variables at each step.

![](https://miro.medium.com/max/1400/1*No7aLXmvJrnneUK8DaET8A.png)

This results in a variety of trees. The variety is what makes a random forest more effective than individual decisions trees.

**We‚Äôve known how to create a random forest. How to use them?**

*   First, we get a new patient and want to know if they have heart disease or not. So take the data and run it down the first tree that we made.  The first tree says ‚ÄúYes‚Äù.
*   Run it down the 2nd tree. The 2nd tree says ‚ÄúYes‚Äù‚Ä¶
*   Repeate for all the trees we made. After running the data down all of the trees in the random forest, we see which option received more votes. If ‚ÄúYes‚Äù received the most votes, then we conclude the patient has heart disease.

**Bootstrapping the data + using the aggregate to make a decision** is called **Bagging.** Bagging is an ensemble algorithm that fits models on different subsets of a training dataset, then combines the predictions from all models. Random forest is an extension of bagging that also randomly selects subsets of features used in each data sample.

Typically, about 1/3 of the original data does not end up in the bootstrapped dataset. This is called **‚ÄúOut-of-Bag‚Äù dataset**. We can run it through all of the trees that were built without it. **Utimately, we can measure how accurate our random forest is by the proportion of OOB samples that have been correctly classified by the random forest.**

How many variables to use?

We can compare the OOB error for a random forest built using only 2 variables per step to random forest to a random forest built using only 3 variables per step.

Therefore,

1.  Build a random forest
2.  Estimate the accuracy of a random forest.
3.  Change the # of variables used per step and repeat step 1. Do this a bunch of times and then choose the one that is most accurate. **Typically, we start by using the square root of the variables and try a few settings above and below that value.**

2.2 Missing Data & Clustering
-----------------------------

Random forest consider 2 types of missing data

(1) Missing data in the original dataset used to create the random forest

(2) Missing data in a new sample that you want to categorize

**üëÄ _(1) Missing data in the original dataset used to create the random forest_**
----------------------------------------------------------------------------------

The general idea for dealing with missing data in (1) is to make an initial guess that could be bad, then gradually refine the guess until it‚Äôs (hopefully) a good guess.

**üåü Initial Guess:**

The initial, and possibly bad guess for Blocked Arteries is just the most common value for Blocked Arteries found in other samples that DO NOT have heart disease.

Example:

![](https://miro.medium.com/max/1400/1*8OpJHd72wwNJhMf01HnFPw.png)

So ‚ÄúNo‚Äù is the initial guess.

Since Weight 4 is numeric initial guess will be the average value of the patients DO NOT have heart disease. (125+150)/2 = 167.5.

üåü **Refine Guess**

Determine which samples are similar to one with missing data. To determine similarity

(1) Build a random forest

(2) Run all of the data down all of the trees.

e.g. Sample 3 and sample 4 both ended up at the same leaf node => They are similar.

We keep track of similar samples using a ‚Äúproximity matrix‚Äù.

![](https://miro.medium.com/max/1400/1*OK7cBfTb1THxT42cq8jpRg.png)

Because no other pair of samples ended in the same leaf node, our proximity matrix looks like this after running down the first tree„ÄÇ

Run all the data down the 2nd tree. Samples 2,3,4 all ended up in the same leaf node

![](https://miro.medium.com/max/1096/1*hJqOylqDozrA6O5j_J2xPQ.png)

Ultimately, say the proximity matrix looks like

![](https://miro.medium.com/max/1072/1*__hjHsz5LFHWBNnIKMAPdg.png)

Then we divide each proximity value by the total # of the trees (assume 10 trees)

![](https://miro.medium.com/max/1400/1*pk5dOnIZRZBsvvkPnR_Kiw.png)

For **Blocked Arteries**, we use the weighted frequency of Yes and No, using proximity values as weights. (1 yes, 2 no => Yes = 1/3, no 2/3)

*   The weight frequency for Yes = 1/3 \* weight for Yes = 1/3 \* (Proximity of Yes/All Proximity) = 1/3 \* (0.1 (sample 2)/(0.1+0.1+0.8)) = 0.03
*   The weight frequency for No = 1/3 \* weight for No = 1/3 \* (Proximity of No/All Proximity) = 2/3 \* ((0.1+0.8)/(0.1+0.1+0.8)) = 0.6
*   Since No has a higher frequency, the missing value = No for sample 4 Blocked Arteries.

For **Weight**, weighted average = 125\*0.1 /(0.1+0.1+0.8) + 180 \*0.1 + 120\*0.8 = 198.5

Now we‚Äôve revised our guess a little bit. We do the whole thing over again. We build a random forest, run the data through the trees, recalculate the proximities and recalculate the missing values. Do this 6‚Äì7 times until the missing values converge (i.e. no longer change each time we recalculate).

‚ùï Something cool with proximity matrix

Proximity matrix after divided each value by the # of trees in the random forest (=10)

![](https://miro.medium.com/max/1400/1*vRr2FVporAcjdz_wLpeb4A.png)![](https://miro.medium.com/max/1400/1*-I-o3MylK_ptqKLMKk0icw.png)

No matter what the data area (ranks, multiple choice, numeric, etc), if we can use it to make a tree, we can draw a heatmal/an MDS plot to show how the samples are related to each other.

üëÄ **_(2) Missing data in a new sample that you want to categorize_**
---------------------------------------------------------------------

Imagine we had already built a random forest with existing data and wanted to classify a new patient

![](https://miro.medium.com/max/1400/1*_VIQGdUHrZ6R0Zy0SbgkSA.png)

Step 1: Create two copies of data, one w/ Heart Disease, one w/o Heart Disease

Step 2: Make a good guess about the missing value: Plug in the most common value/median value for all observations in the training set with the same label. We can use (1) iterative method to refine guess (2) or just run the two copies down the tree

Step 3: Then run the samples down the tree in the forest

Step 4: See which of the two is correctly labeled by the random forest the most time

*   Option 1 was correctly labeled ‚ÄúYes‚Äù in all 3 times
*   Option 2 was correctly labeled ‚ÄúNo‚Äù in 1 time.
*   So we choose Option 1.

2.3 Random Forest in R
----------------------

*   \# of variables tried at each split: classification tree has a default setting sqrt(# of variables), regression tree has a default setting sqrt(# of variables/3)
*   Optimization: Add # of trees, use option # of variables (write a loop)

