4\. Gradient Boost (GB)
=======================

4.1 Regression Main Ideas
-------------------------

**4.1.1 Dataset**

![](https://miro.medium.com/max/1076/1*JIxGyXg1UPEg5D3LtOntfA.png)

GB starts by making a single leaf, instead of a tree or stump. This leaf represents an initial guess for the Weights of all of the samples. When trying to predict a continuous value like Weight, the first guess is the average value. Then GB builds a tree.

**4.1.2 Difference and Similarities between AdaBoost and Gradient Boost**

*   Like AdaBoost, GB builds fixed-sized trees based on previous tree’s errors
*   Unlike Adaboost, each tree is usually **larger than a stump**. That said, GB still restricts the size of the tree. In practice, people often set the maximum number of leaves to be between 8 and 32.
*   Also like AdaBoost, GB scales the trees. However, GB scales all trees by the same amount. Then GB builds another tree based on the errors made by the previous tree and then it scales the tree. GB continues to build trees in this fashion until it has made the number of trees you asked for, or additional trees fail to improve the fit.

**4.1.3 Example**

**Step 1: Calculate the average weight** (=71.2). This is the first attempt at predicting everyone’s weight. In other words, if we stopped right now, we would predict that everyone weighed 71.2 kg. However, GB doesn’t stop here.

**Step 2: Build a tree based on the errors from the first tree.** The errors that the previous tree made are the **differences between the observed weights and the predicted weight** (residual)

Step 3:

![](https://miro.medium.com/max/1400/1*5kAVPdN8YleoccGdWa-CyQ.png)

\=>

![](https://miro.medium.com/max/1400/1*xJKUpyuXSqrv43Y0pA_Dpg.png)

In this example, we are only allowing up to 4 leaves. By restricting the total number of leaves we get fewer leaves than residuals. There are two rows of data that go to the same leaf. We replace these residuals with their average. (-14.2–15.2)/2 = -14.7, (1.8+5.8)/2 = 3.8.

Now we can **combine the original leaf with the new tree to make a new prediction of an individual’s weight from the training data.**

Start with the initial prediction 71.2,

![](https://miro.medium.com/max/1400/1*7HuvrMhxU1ezaHkU_TYrBg.png)

so the Predicted Weight = 71.2 + 16.8 = 88, which is the same as the Observed Weight. The model fits the training data too well! (Low bias, High variance). GB deals with this problem by using a **Learning Rate (in \[0,1\])** to scale the contribution from the new tree.

![](https://miro.medium.com/max/1400/1*g7u_rGrmC7RBGCnhWlF-kg.png)

But it’s a little better than 71.2 (the Prediction made with just the original leaf). In other words, scaling the tree by the learning rate results in a small step in the right direction.

According to the dude that invented gradient boost, Jerome Freedman , empirical evidence shows that taking lots of small steps in the right direction results in better predictions with a testing data set, i.e. **lower variance.**

Step 4: Build another tree so we can take another small step in the right direction

(1) Calculate the residuals: Observed — Predicted

Now the predicted changed. 88–(71.2 + 0.1\*16.8) = 15.1

![](https://miro.medium.com/max/1400/1*iEzjTWS6uRasJ7YAYtKHxQ.png)

Step 5: Combine the new tree with the previous tree and the initial leaf. Scale all of the trees by the learning rate (e.g. 0.1) and add everything together.

E.g. We want to make a prediction

![](https://miro.medium.com/max/1212/1*QvEgreEb8tV_lLJgPSTeWg.png)![](https://miro.medium.com/max/1400/1*sifhBKlkR5GPa09KZD_StA.png)![](https://miro.medium.com/max/1400/1*mHll7s7xpKjGb49d4Z2-4Q.png)

**4.1.4 Summary**

When GB is used for regression,

*   we start with a leaf that is the average value of the variable we want to predict,
*   then we add a tree based on the residuals (the difference between the observed values and the predicted values)
*   and we scale the tree's contribution to the final prediction with a learning rate
*   then we add another tree based on the new residuals
*   keep adding trees based on the errors made by the previous tree

4.2 Regression Details
----------------------

**4.2.1 Dataset**

![](https://miro.medium.com/max/1400/1*hru49tQTJxj5DL9YCi-EBQ.png)

**4.2.2 Algorithm**

![](https://miro.medium.com/max/1400/1*UEuqeOfVMyC7VtyvdDHitQ.png)

**4.2.3 Algorithm Detail**

*   Input: Data, Loss function

A popular loss function for regression: 1/2 \* (Observed — Predicted)². We use 1/2 because

![](https://miro.medium.com/max/1400/1*SHvIA4twnAvWG1QfO6J9ew.png)

*   Step 1:

![](https://miro.medium.com/max/1400/1*hct0mJtwXUeH4iliKWmvIQ.png)![](https://miro.medium.com/max/1400/1*X5Y-ahtaCH00qGQgYUR1rA.png)![](https://miro.medium.com/max/1400/1*Etyle_qzacgtb-rCQrGEFw.png)

*   Step 2:

![](https://miro.medium.com/max/1400/1*UAZX2vbpTmYCa1ZOWZKeeQ.png)

Make M trees (e.g. M = 100):

A) Computer residual for each sample, i = sample number (observations), m = tree number

B) Build a regression tree to predict the residuals instead of the Weights (label), Leaves are the “terminal regions”.

C) For each leaf in the new tree, taking the previous prediction into account, compute the output values for each leaf.

![](https://miro.medium.com/max/1400/1*OlZJRLYRy4HHmqMgnHK0Tw.png)

D) Make a new prediction for each sample based on the previous prediction, the learning rate, and the output values from the new tree.

![](https://miro.medium.com/max/1400/1*3IoCzL7O4mDCyVblqAaAxw.png)

nu: Learning rate. A small learning rate reduces the effect each tree has on the final prediction, and this improves accuracy in the long run.

*   Step 3:

![](https://miro.medium.com/max/1400/1*HhPy3vaVGCGJ1COsCdaSlw.png)

4.3 Classification
------------------

Training Data:

![](https://miro.medium.com/max/1124/1*wCxliwJBqAComPkKfdi0fg.png)

*   When we use GB for Classification, the initial Prediction for every individual is the **log(odds)**
*   Think of the log(odds) as the Logistic Regression equivalent of the average

![](https://miro.medium.com/max/1400/1*uIQ07p3NegInX8wUsLuWrQ.png)

*   log(4/2) = 0.6931 <- Initial prediction
*   Probability of Loving Troll 2 = 0.6631. Since the probability of Loving Troll 2 is greater than 0.5, we can Classify everyone in the training Dataset as someone who Loves Troll 2. But such classification is pretty lame. Because two of the people do not love the movie.

![](https://miro.medium.com/max/1400/1*ftHtanmonHFCZA18oIHDVg.png)

Calculate residuals. Then we use Likes Popcorn, Age, Favorite Color to predict Residual.

*   When we used GB for Regression, a leaf with a single Residual had an Output Value equal to that Residual.
*   But it’s more complex using GB for Classification. Since the leaf is derived from a Probability, so we can’t just add them together to get a new log(odds) Prediction without some sort of transformation

![](https://miro.medium.com/max/1400/1*UpdEL58VCTMgtq-Z4d_rpA.png)![](https://miro.medium.com/max/1400/1*08d3Szb1hiYN7xeFC7_PSg.png)

*   Numerator: sum of all of the Residuals in the leaf
*   Denominator: Sum of the previously predicted probabilities for each residuals \* (1-same predicted probability)

![](https://miro.medium.com/max/1400/1*Y4_YDTwWbPXRPwKrhMyxRw.png)![](https://miro.medium.com/max/1400/1*gZC35BUk5hCHxFAONIs-2A.png)

*   Plug in
*   Convert the new log(odds) Prediction into a Probability. e^(1.8)/1+e^(1.8) = 0.9
*   Calculated predicted probability
*   Calculate residual
*   Calculate output value

Summary:

1.  Start with just a leaf, made one prediction for every individual
2.  Build a tree based on the Residuals, the difference between the observed values and the single value predicted by the leaf
3.  Calculate the Output value by each leaf, scaled by learning rates
4.  Then we built another tree based on the new Residuals, the difference between the observed values and the values **predicted by the leaf & the first tree**
5.  Calculate the output value for each tree, scaled by the learning rate
6.  The process repeats until we have made the **maximum number of trees specified, or the residuals get super small.**

![](https://miro.medium.com/max/1400/1*xplmOtP9jklIZLCfwGPgZA.png)

Test data:

Convert log(odds) into probability (=0.9), 0.9>0.5, so label = Yes

Note: GB usually uses trees with between 8–32 leaves

4.4 Classification Details
--------------------------

![](https://miro.medium.com/max/1400/1*pH8quaqQi4REaxhg19yMvw.png)Algorithm![](https://miro.medium.com/max/1400/1*JnqYgOIdSNXPYS-e-y4IvQ.png)![](https://miro.medium.com/max/1400/1*ZZWt4zFv-ybArn3bTrn3Zw.png)![](https://miro.medium.com/max/1400/1*bjPo1YqbbKfIJ7goVmRWjQ.png)

Loss function

Want to show if it’s differentiable

Step 1

![](https://miro.medium.com/max/1400/1*meXPxoVS05mYtAFQnHl43A.png)

We have created the initial predicted log(odds), F\_0(x) = log(2/1) = 0.69. In other word, we created a leaf that predicts the log(odds) that someone will love Troll 2 = 0.69

![](https://miro.medium.com/max/1400/1*DkIlePZKLkO0sI0vfCp69A.png)![](https://miro.medium.com/max/1400/1*yFRXAG76ApPE5_hHpJFIRw.png)(A) Pseudo Residual = Observed Probability — Predicted Probability, convert the pseudo residue to probability.![](https://miro.medium.com/max/1400/1*AUj3At4Is3o_oparntuqgQ.png)(B) Build a regression tree using Likes Popcorn, Age, Favorite Color to predict Residual![](https://miro.medium.com/max/1400/1*eq_Wtn9Vo-5t27X3BZJonw.png)C), (D) Calculate output value for the new tree![](https://miro.medium.com/max/1400/1*zwPREfbo7pVK3gQ8fLfogA.png)Probability = 0.97 > 0.5. This person loves Troll 2.

