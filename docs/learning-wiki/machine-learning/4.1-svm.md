# Support Vector Machines

### Formulating a Decision Rule

- Suppose we have a dataset with positive and negative classes:

![](img/4.1.1.png#center){width=60%}

- We can draw an arbitrary decision boundary:

![](img/4.1.2.png#center){width=60%}

- In order to classify new points using this decision boundary, we define a vector, $\overrightarrow w$, that is perpendicular to the decision boundary:

![](img/4.1.3.png#center){width=60%}

- When classifying a given data point, $u$, we can interpret it as a vector $\overrightarrow u$ and take the dot product between $\overrightarrow w$ and $\overrightarrow u$ ($\overrightarrow u \cdot \overrightarrow w$)
- If this dot product is greater than a constant $c$, then the data point, $u$ can be classified as positive:

$$
\overrightarrow u \cdot \overrightarrow w \ge c \implies
$$

$$
\overrightarrow u \cdot \overrightarrow w + b \ge 0 \tag{1}
$$

- We have not defined enough constraints in order to calculate the constant, $c$, or the magnitude of $w$; this means we have to formulate some constraints


### Calculate an Equation for the Margins

- Suppose we are given a data point that we know is either negative or positive, we can create constraints where:

$$
\overrightarrow w \cdot \overrightarrow x_+ + b \ge 1
$$

$$
\overrightarrow w \cdot \overrightarrow x_- + b \le -1
$$

- For convenience, we can define a variable $y_i$ such that $y_i = +1$ for positive samples and $y_i = -1$ for negative samples in order to combine the above equations:

$$
y_i (\overrightarrow w \cdot \overrightarrow x_i + b) \ge 1 \implies y_i (\overrightarrow w \cdot \overrightarrow x_i + b) -1 \ge 0
$$

- Notice, when $y_i = -1$ and we move it to the other side, the $\ge$ flips and $1\rightarrow-1$

- Now that we have this inequality, we notice that when:

$$
y_i (\overrightarrow w \cdot \overrightarrow x_i + b) -1 = 0 \tag{2}
$$

- We get an equation for our margins

![](img/4.1.2.png#center){width=60%}


### Calculating the Width of our Decision Boundary/Margins

- Looking back to our goal of creating a decision boundary, it is clear that we want a boundary that is as wide as possible
- To do so, we must be able to calculate its width
- In order to determine the distance between our two margins, we first think of two arbitrary vectors $\overrightarrow{x_+}$ and $\overrightarrow{x_-}$ that lie on the positive and negative margins, respectively:

![](img/4.1.4.png#center){width=60%}

- We can calculate the vector going from $\overrightarrow{x_-}$ to $\overrightarrow{x_+}$ as $\overrightarrow{x_+} - \overrightarrow{x_-}$
- Given the unit vector $\hat w$ that is perpendicular to the decision boundary, the width between the margins is:

$$
\text{width} = \hat w \cdot (\overrightarrow{x_+} - \overrightarrow{x_-})
$$

$$
\text{where: } \hat w = \frac{\overrightarrow w}{||w||}
$$

- In order to solve for width, we can take equation $(2)$ and solve for $x_+$ and $x_-$ to get:

$$
x_+ = \frac{1 - b}{\overrightarrow w}, x_- = \frac{1 + b}{\overrightarrow w}
$$

- When subbing this back into the width equation we get:

$$
\text{width} = \hat w \cdot (\frac{2}{\overrightarrow w}) = \frac{\overrightarrow w}{||w||} \cdot \frac{2}{\overrightarrow w} = \frac{2}{||w||}
$$

- Thus, we want to maximize this width term: $\frac{2}{||w||}$
    - This is equivalent to maximizing $\frac{1}{||w||}$
    - Or minimizing $||w||$
    - Or minimizing $\frac12||w||^2$

- Thus we want:

$$
\text{argmin}_w \frac12||w||^2 \tag{3}
$$


### Optimization

- To do so we use Lagrange Multipliers:

$$
L = \frac12||w||^2 - \sum_i\alpha_i[y_i (\overrightarrow w \cdot \overrightarrow x_i + b) -1]
$$

- We try to find the extremum:

$$
\frac{\partial L}{\partial w} = 0
$$

$$
\frac{\partial L}{\partial w} = w- \sum_i\alpha_iy_ix_i
$$

$$
w = \sum_i\alpha_iy_ix_i
$$

- From this, we know that w is simply a linear sum of the samples!

- We also must differentiate to other variables that may change i.e. $b$

$$
\frac{\partial L}{\partial b} = 0
$$

$$
\frac{\partial L}{\partial b} = \sum_i\alpha_iy_i
$$

$$
0 = \sum_i\alpha_iy_i
$$

- Subbing this back into the original equation $L$

$$
L = \frac12||w||^2 - \sum_i\alpha_i[y_i (\overrightarrow w \cdot \overrightarrow x_i + b) -1]
$$

$$
L = \frac12(\sum_i\alpha_iy_ix_i)(\sum_j\alpha_jy_jx_j) - \sum_i\alpha_iy_i\overrightarrow x_i (\sum_j\alpha_jy_jx_j)- b\sum_i\alpha_iy_i + \sum_i\alpha_i
$$

$$
\text{since: } \sum_i\alpha_iy_i = 0
$$

$$
L = \frac12(\sum_i\alpha_iy_ix_i)(\sum_j\alpha_jy_jx_j) - \sum_i\alpha_iy_i\overrightarrow x_i (\sum_j\alpha_jy_jx_j) + \sum_i\alpha_i
$$

$$
L = \sum_i\alpha_i - \frac12(\sum_i\alpha_iy_ix_i)(\sum_j\alpha_jy_jx_j)
$$

$$
L = \sum_i\alpha_i - \frac12\sum_i\sum_j\alpha_iy_i\alpha_jy_jx_i \cdot x_j
$$

- Notice, the optimization only depends on the summation of dot products between pairs of samples!

### Bringing all of this Together

- To recap of all the work we have done, we first have a "Decision Rule"
- This is the computation we perform to check if a given data point, $\overrightarrow u$, should be classified as positive

$$
\overrightarrow u \cdot \overrightarrow w + b \ge 0 \tag{1}
$$

- As $\overrightarrow w$ and $b$ are parameters for the decision boundary which defines our "Decision Rule", we must optimize them in some manner
- We do so by defining two symmetrical margins parallel to our decision boundary, that are still able to separate our two classes
    - We calculate the equation for this boundary to be equation $(2)$

![](img/4.1.3.png#center){width=60%}

- We want to maximize the distance between these margins and our decision boundary
- To do so, we find the equation for this distance, equation $(3)$

- Using equations $(2)$ and $(3)$